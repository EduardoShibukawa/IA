#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon May  8 12:49:58 2017

@author: eduardo
"""
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('voice.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
columns_name = dataset.columns[:-1]


    
# Encoding categorical data
from sklearn.preprocessing import LabelEncoder
# Encoding the Dependent Variable
labelencoder_y = LabelEncoder()
y = labelencoder_y.fit_transform(y)
                
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#### Feature Selection SelectPercentile
from sklearn import feature_selection;

select_percentile = feature_selection.SelectPercentile(feature_selection.f_classif)
select_percentile.fit(X_train, y_train)

scores_sp = select_percentile.scores_
indices_sp = np.argsort(scores_sp)[::-1]

# Build a forest and compute the feature importances
from sklearn.ensemble import ExtraTreesClassifier

forest = ExtraTreesClassifier(n_estimators=250)
forest.fit(X_train, y_train)
importances_f = forest.feature_importances_
std_f = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
mean_f = np.mean([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices_f = np.argsort(importances_f)[::-1]

# Using RFE
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
 
#use linear regression as the model
lr = LinearRegression()
#rank all features, i.e continue the elimination until the last one
rfe = RFE(lr, n_features_to_select=1)
rfe.fit(X_train, y_train)

# Feature Selection 
import math
from sklearn.svm import SVC
from sklearn.feature_selection import RFECV

#rank all features, i.e continue the elimination until the last one
svc = SVC(kernel='linear')
rfecv = RFECV(estimator=svc, cv=10, scoring='accuracy', n_jobs=-1)
rfecv.fit(X, y)

### PLOTING

print("------------------------------------------------")
print("Features sorted by their rank(SelectPercentile):")
print(sorted(zip(indices_sp + 1, map(lambda x: columns_name[indices_sp[x]], indices_sp))))

print("--------------------------------------")
print("Features sorted by their rank(Forest):")
print(sorted(zip(indices_f + 1, map(lambda x: columns_name[indices_f[x]], indices_f))))

print("--------------------------------------")
print("Features sorted by their rank(RFE\LR):")
print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), columns_name)))

print("-----------------------------------------")
print("Features sorted by their rank(RFECV\SVC):")
print(sorted(zip(map(lambda x: round(x, 4), rfecv.ranking_), columns_name)))

n_groups = len(columns_name)
y_sp = [x[1] for x in sorted(zip(map(lambda x:indices_sp[x], indices_sp), indices_sp + 1))]
y_f = [x[1] for x in sorted(zip(map(lambda x:indices_f[x], indices_f), indices_f + 1))]
y_rfe = rfe.ranking_
y_rfecv = rfecv.ranking_
    
 
